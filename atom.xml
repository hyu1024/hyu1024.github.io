<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>HBlog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-04-03T19:59:06.687Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>H Yu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Intro to AI 03</title>
    <link href="http://yoursite.com/2020/04/02/Intro-to-AI-03/"/>
    <id>http://yoursite.com/2020/04/02/Intro-to-AI-03/</id>
    <published>2020-04-02T20:02:27.850Z</published>
    <updated>2020-04-03T19:59:06.687Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Bayes-Theorem-and-Naive-Bayes"><a href="#Bayes-Theorem-and-Naive-Bayes" class="headerlink" title="Bayes Theorem and Naive Bayes"></a>Bayes Theorem and Naive Bayes</h1><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li>What is Bayes theorem and how is it applied to various real-life scenarios</li><li>What is Naive Bayes and how is it used in machine learning</li><li>What is continuous Naive Bayes is and how is it used in machine learning</li></ul><h2 id="Bayes-Theorem-everywhere-in-our-life"><a href="#Bayes-Theorem-everywhere-in-our-life" class="headerlink" title="Bayes Theorem everywhere in our life"></a>Bayes Theorem everywhere in our life</h2><blockquote><p>What the chance of getting wet on our way to the university?</p></blockquote><h2 id="Probability"><a href="#Probability" class="headerlink" title="Probability"></a>Probability</h2><p>Probability gives a numerical description of how likely an event is to occur:</p><ul><li><p>Probability of event(outcome)A:P(A) </p><ul><li>P(rainy) = 0.3</li><li>P(snowy) = 0.05</li><li>P(a coin lands on its head) = 0.5</li></ul></li><li><p>P(A) + P(^A)=1.0</p><ul><li>P(rainy) + P(notrainy)=1,so P(notrainy)=0.7</li></ul></li></ul><h2 id="Random-variables"><a href="#Random-variables" class="headerlink" title="Random variables"></a>Random variables</h2><p>A random variable is avariable to describe the outcome of a random experiment</p><p>Consider a discrete random variable Weather </p><ul><li>P(Weather = rainy) = 0.3</li><li>P(Weather = snowy) = 0.05</li><li>P(Weather = sunny) = 0.4</li><li>P(Weather = cloudy) = 0.25</li></ul><p>Sum of the probabilities of all the outcomes is 1</p><script type="math/tex; mode=display">\sum { }_{Weather~ \in ~\{rainy,~sonwy,~sunny,~cloudy\}} P(Weather) = 1</script><h2 id="Probability-of-multiple-random-variables"><a href="#Probability-of-multiple-random-variables" class="headerlink" title="Probability of multiple random variables"></a>Probability of multiple random variables</h2><p>We may want to know the probability of two or more events occurring, e.g.,</p><ul><li>P(Weather = rainy, Season = winter) = 0.1</li><li>P(Weather = rainy, Season = spring) = 0.15</li><li>P(Weather = snowy, Season = summer) = 0</li></ul><p>Joint probability of events A and B: P(A,B) or P(A B)</p><p><img src="/2020/04/02/Intro-to-AI-03/01.png" alt></p><h2 id="Conditional-Probability"><a href="#Conditional-Probability" class="headerlink" title="Conditional Probability"></a>Conditional Probability</h2><p>We may also be interested in the probability of an event given the occurrence of another event: P(A given B) or P(A $\mid$ B)</p><ul><li>P(Weather = rainy $\mid$ Season = winter) = 0.2</li><li>P(Weather = rainy $\mid$ Season = spring) = 0.3</li><li>P(Weather = snowy $\mid$ Season = winter) = 0.5</li></ul><script type="math/tex; mode=display">P(A \mid B) = \frac{P(A,B)}{P(B)}</script><script type="math/tex; mode=display">P(A \mid B) + P( \hat A \mid B) = 1</script><script type="math/tex; mode=display">P(A \mid B) = \frac{P(A,B)}{P(B)} \Longrightarrow P(A,B) = P(A \mid B) P(B)</script><h3 id="Calculating-a-conditional-probability-without-the-joint-probability"><a href="#Calculating-a-conditional-probability-without-the-joint-probability" class="headerlink" title="Calculating a conditional probability without the joint probability"></a>Calculating a conditional probability without the joint probability</h3><script type="math/tex; mode=display">P(A \mid B) = \frac{P(A) \cdot P(B \mid A)}{P(B)}</script><p>Where:</p><ul><li>$A$: Hypothesis</li><li>$B$: Evidence</li><li>$P(A \mid B)$: Posterior Probability</li><li>$P(A)$: Prior Probability</li><li>$P(B \mid A$: Likelihood</li><li>$P(B)$: Evidence Probability</li></ul><script type="math/tex; mode=display">P(H \mid E) = \frac{P(H) \cdot P(E \mid H)}{P(E)}</script><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>What the chance of getting wet on our way to the university?</p><ul><li>Probability of raining at Birmingham in March is 0.4</li><li>Probability of taking an umbrella when raining is 0.9</li><li>Probability of taking an umbrella when not raining is 0.2</li></ul><h3 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h3><p>$P(raining \mid \hat {umbrella}) = ?$</p><h3 id="Analyse"><a href="#Analyse" class="headerlink" title="Analyse"></a>Analyse</h3><ul><li>$H = raining,\: E = \hat{umbrella}$</li><li>$P(raining) = 0.4, \: P(\hat {raining} = 0.6)$</li><li>$P(umbrella \mid raining) = 0.9, \: P(\hat{umbrella}\mid raining) = 0.1$</li><li>$P(umbrella\mid \hat{raining}) = 0.2, \: P(\hat{umbrella} \mid \hat{raining}) = 0.8$</li></ul><p>$\therefore \: $ </p><script type="math/tex; mode=display">\begin{aligned}        &P(raining \mid \hat {umbrella}) \\\\= \quad &P(raining) \cdot P(\hat{umbrella}\mid raining) \: / \: P(\hat{umbrella})\\\\= \quad &0.4 \times 0.1  \: / \:  P(\hat{umbrella})\\\\\approx \quad &0.08\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}P(\hat{umbrella}) &\quad= P(raining) \cdot P(\hat{umbrella} \mid raining) + P(\hat{raining})\cdot P(\hat{umbrella}\mid \hat{raining}) \\\\&\quad= 0.4 \times 0.1 + 0.6 \times 0.8 \\\\&\quad= 0.52\end{aligned}</script><h2 id="Bayes-Theorem"><a href="#Bayes-Theorem" class="headerlink" title="Bayes Theorem"></a>Bayes Theorem</h2><script type="math/tex; mode=display">\begin{aligned}P(E) &\quad = P(H) \cdot P(E \mid H) + P(\hat{H})\cdot P(E\mid \hat{H}) \\\\&\quad= P(H,E) + P(\hat{H},E)\end{aligned}</script><script type="math/tex; mode=display">P(H\mid E) = \frac {P(H)P(E\mid H)}{P(H) \cdot P(E \mid H) + P(\hat{H})\cdot P(E\mid \hat{H})}</script><h2 id="Example-2-Drug-Testing"><a href="#Example-2-Drug-Testing" class="headerlink" title="Example 2: Drug Testing"></a>Example 2: Drug Testing</h2><p>Suppose that a drug test will produce 99% true positive results for drug users and 99% true negative results for non-drug users. Suppose that 0.5% of people are users of the drug. <strong>What is the probability that a randomly selected individual with a positive test is a drug user? What is the probability that a randomly selected individual with a negative test is a drug user?</strong></p><ul><li>Drug user: $drug$</li><li>Non-drug user: $\hat{drug}$</li><li>Positive: $+$</li><li>Negative: $-$</li></ul><h3 id="Question-1"><a href="#Question-1" class="headerlink" title="Question"></a>Question</h3><ul><li>$P(drug\mid +) = ?$</li><li>$P(drug\mid -) = ?$</li></ul><h3 id="Analyse-1"><a href="#Analyse-1" class="headerlink" title="Analyse"></a>Analyse</h3><p>$P(drug\mid +) = ?$</p><ul><li><p>$P(drug\mid +) = P(drug) \cdot P(+\mid drug) \:/\: P(+)$</p></li><li><p>$P(+) = P(drug) \cdot P(+\mid drug) + P(\hat{drug}) \cdot P(+ \mid \hat{drug})=1.49\%$</p></li><li><p>$P(drug\mid +) = 0.5\% \times 99\% \:/\: 1.49\% \approx 33.2\%$</p></li></ul><p>$P(drug\mid -) = ?$</p><ul><li><p>$P(drug\mid -) = P(drug) \cdot P(-\mid drug) \:/\: P(-)$</p></li><li><p>$P(-) = P(drug) \cdot P(-\mid drug) + P(\hat{drug}) \cdot P(- \mid \hat{drug}) = 98.51\%$</p></li><li><p>$P(drug\mid -) = 0.5\% \times 1\% \:/\: 98.51\% \approx 0.005\%$</p></li></ul><h2 id="Revisit-Drug-Testing-Extension"><a href="#Revisit-Drug-Testing-Extension" class="headerlink" title="Revisit Drug Testing - Extension"></a>Revisit Drug Testing - Extension</h2><p>Suppose that a drug test will produce 99% true positive results for drug users and 99% true negative results for non‐drug users. Suppose that 0.5% of people are users of the drug. <strong>What is the probability that a randomly selected individual with two positive tests is a drug user?</strong> </p><p><em>(assume that these two tests are mutually independent given whether if the person is a drug user or not)</em></p><h3 id="Question-2"><a href="#Question-2" class="headerlink" title="Question"></a>Question</h3><p>$P(drug\mid ++) = ?$</p><h3 id="Analyse-2"><a href="#Analyse-2" class="headerlink" title="Analyse"></a>Analyse</h3><ul><li><p>$P(drug\mid ++) = P(drug) \cdot P(++\mid drug) \:/\: P(++)$</p></li><li><p>$P(++\mid drug) = P(+\mid drug) \cdot P(+\mid drug) = 99\% \cdot 99\% = 98.01\%$</p></li><li><p>$P(++) = P(drug)\cdot P(++\mid drug) + P(\hat{drug})\cdot P(++\mid \hat{drug}) = 0.5\%$</p></li><li><p>$P(drug\mid ++) = 98.01\%$</p></li></ul><h2 id="Interpretation-of-Bayes-Theorem"><a href="#Interpretation-of-Bayes-Theorem" class="headerlink" title="Interpretation of Bayes Theorem"></a>Interpretation of Bayes Theorem</h2><ul><li>Probability measures a “degree of belief”</li><li>Bayes theorem links the degree of belief in a hypothesis (proposition) before and after accounting for evidence</li><li>$P(H \mid E) = P(H) \cdot P(E \mid H)\:/\:P(E)$, hypothesis $H$ and evidence $E$</li><li>$P(H)$ is the initial degree of belief in $H$ (i.e. prior probability)</li><li>$P(H\mid E)$ is the degree of belief having accounted for $E$</li><li>Bayes theorem is not to determine our belief, but to update our belief</li></ul><h2 id="Bayes-Theorem-for-Classification"><a href="#Bayes-Theorem-for-Classification" class="headerlink" title="Bayes Theorem for Classification"></a>Bayes Theorem for Classification</h2><ul><li>A machine learning model is to predict the relationships in the data, such as the relationship between features (i.e. attributes) (e.g. round, juicy, orange skin) and classes (e.g. orange, apple)</li><li>Bayes theorem provides a probabilistic way to model the relationship in data: evidence = feature (input), hypothesis = class (output)</li><li>It calculates the probability of an object given a number of<br>features (input $f_1,f_2,…, f_n$) belonging to a particular class (output $c$): $P(Y=c \mid X_1 = f_1, X_2 = f_2,…, X_n = f_n)$</li></ul><h2 id="Example-Tooth-cavity-testing"><a href="#Example-Tooth-cavity-testing" class="headerlink" title="Example: Tooth cavity testing"></a>Example: Tooth cavity testing</h2><ul><li>To predict whether people have cavity when they do not do mouth wash by Bayes theorem</li><li>Difference from the previous examples: unknown prior probability, likelihood, etc; need to learn from training examples</li></ul><h3 id="Analyse-3"><a href="#Analyse-3" class="headerlink" title="Analyse"></a>Analyse</h3><ul><li>Input variable: Wash</li><li>Output variable: Cavity</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Bayes-Theorem-and-Naive-Bayes&quot;&gt;&lt;a href=&quot;#Bayes-Theorem-and-Naive-Bayes&quot; class=&quot;headerlink&quot; title=&quot;Bayes Theorem and Naive Bayes&quot;&gt;&lt;/a
      
    
    </summary>
    
    
      <category term="AI" scheme="http://yoursite.com/categories/AI/"/>
    
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
      <category term="Bayes" scheme="http://yoursite.com/tags/Bayes/"/>
    
  </entry>
  
</feed>

<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>HBlog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-04-04T00:09:13.564Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>H Yu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Intro to AI 03</title>
    <link href="http://yoursite.com/2020/04/02/Intro-to-AI-03/"/>
    <id>http://yoursite.com/2020/04/02/Intro-to-AI-03/</id>
    <published>2020-04-02T20:02:27.850Z</published>
    <updated>2020-04-04T00:09:13.564Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Bayes-Theorem-and-Naive-Bayes"><a href="#Bayes-Theorem-and-Naive-Bayes" class="headerlink" title="Bayes Theorem and Naive Bayes"></a>Bayes Theorem and Naive Bayes</h1><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li>Bayes theorem </li><li>Naive Bayes</li><li>Naive Bayes in machine learning</li><li>Naive Bayes for continuous input attributes</li><li>Advantages and disadvantages</li><li>Applications</li></ul><h2 id="Bayes-Theorem-everywhere-in-our-life"><a href="#Bayes-Theorem-everywhere-in-our-life" class="headerlink" title="Bayes Theorem everywhere in our life"></a>Bayes Theorem everywhere in our life</h2><blockquote><p>What the chance of getting wet on our way to the university?</p></blockquote><h2 id="Probability"><a href="#Probability" class="headerlink" title="Probability"></a>Probability</h2><p>Probability gives a numerical description of how likely an event is to occur:</p><ul><li><p>Probability of event(outcome)A:P(A) </p><ul><li>P(rainy) = 0.3</li><li>P(snowy) = 0.05</li><li>P(a coin lands on its head) = 0.5</li></ul></li><li><p>P(A) + P(^A)=1.0</p><ul><li>P(rainy) + P(notrainy)=1,so P(notrainy)=0.7</li></ul></li></ul><h2 id="Random-variables"><a href="#Random-variables" class="headerlink" title="Random variables"></a>Random variables</h2><p>A random variable is avariable to describe the outcome of a random experiment</p><p>Consider a discrete random variable Weather </p><ul><li>P(Weather = rainy) = 0.3</li><li>P(Weather = snowy) = 0.05</li><li>P(Weather = sunny) = 0.4</li><li>P(Weather = cloudy) = 0.25</li></ul><p>Sum of the probabilities of all the outcomes is 1</p><script type="math/tex; mode=display">\sum { }_{Weather~ \in ~\{rainy,~sonwy,~sunny,~cloudy\}} P(Weather) = 1</script><h2 id="Probability-of-multiple-random-variables"><a href="#Probability-of-multiple-random-variables" class="headerlink" title="Probability of multiple random variables"></a>Probability of multiple random variables</h2><p>We may want to know the probability of two or more events occurring, e.g.,</p><ul><li>P(Weather = rainy, Season = winter) = 0.1</li><li>P(Weather = rainy, Season = spring) = 0.15</li><li>P(Weather = snowy, Season = summer) = 0</li></ul><p>Joint probability of events A and B: P(A,B) or P(A B)</p><p><img src="/2020/04/02/Intro-to-AI-03/01.png" style="zoom:25%;"></p><h2 id="Conditional-Probability"><a href="#Conditional-Probability" class="headerlink" title="Conditional Probability"></a>Conditional Probability</h2><p>We may also be interested in the probability of an event given the occurrence of another event: P(A given B) or P(A $\mid$ B)</p><ul><li>P(Weather = rainy $\mid$ Season = winter) = 0.2</li><li>P(Weather = rainy $\mid$ Season = spring) = 0.3</li><li>P(Weather = snowy $\mid$ Season = winter) = 0.5</li></ul><script type="math/tex; mode=display">P(A \mid B) = \cfrac{P(A,B)}{P(B)}</script><script type="math/tex; mode=display">P(A \mid B) + P( \hat A \mid B) = 1</script><script type="math/tex; mode=display">P(A \mid B) = \cfrac{P(A,B)}{P(B)} \Longrightarrow P(A,B) = P(A \mid B) P(B)</script><h3 id="Calculating-a-conditional-probability-without-the-joint-probability"><a href="#Calculating-a-conditional-probability-without-the-joint-probability" class="headerlink" title="Calculating a conditional probability without the joint probability"></a>Calculating a conditional probability without the joint probability</h3><script type="math/tex; mode=display">P(A \mid B) = \cfrac{P(A) \cdot P(B \mid A)}{P(B)}</script><p>Where:</p><ul><li>$A$: Hypothesis</li><li>$B$: Evidence</li><li>$P(A \mid B)$: Posterior Probability</li><li>$P(A)$: Prior Probability</li><li>$P(B \mid A$: Likelihood</li><li>$P(B)$: Evidence Probability</li></ul><script type="math/tex; mode=display">P(H \mid E) = \cfrac{P(H) \cdot P(E \mid H)}{P(E)}</script><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>What the chance of getting wet on our way to the university?</p><ul><li>Probability of raining at Birmingham in March is 0.4</li><li>Probability of taking an umbrella when raining is 0.9</li><li>Probability of taking an umbrella when not raining is 0.2</li></ul><h3 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h3><p>$P(raining \mid \hat {umbrella}) = ?$</p><h3 id="Analyse"><a href="#Analyse" class="headerlink" title="Analyse"></a>Analyse</h3><ul><li>$H = raining,\: E = \hat{umbrella}$</li><li>$P(raining) = 0.4, \: P(\hat {raining} = 0.6)$</li><li>$P(umbrella \mid raining) = 0.9, \: P(\hat{umbrella}\mid raining) = 0.1$</li><li>$P(umbrella\mid \hat{raining}) = 0.2, \: P(\hat{umbrella} \mid \hat{raining}) = 0.8$</li></ul><p>$\therefore \: $ </p><script type="math/tex; mode=display">\begin{aligned}        &P(raining \mid \hat {umbrella}) \\\\= \quad &P(raining) \cdot P(\hat{umbrella}\mid raining) \: / \: P(\hat{umbrella})\\\\= \quad &0.4 \times 0.1  \: / \:  P(\hat{umbrella})\\\\\approx \quad &0.08\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}P(\hat{umbrella}) &\quad= P(raining) \cdot P(\hat{umbrella} \mid raining) + P(\hat{raining})\cdot P(\hat{umbrella}\mid \hat{raining}) \\\\&\quad= 0.4 \times 0.1 + 0.6 \times 0.8 \\\\&\quad= 0.52\end{aligned}</script><h2 id="Bayes-Theorem"><a href="#Bayes-Theorem" class="headerlink" title="Bayes Theorem"></a>Bayes Theorem</h2><script type="math/tex; mode=display">\begin{aligned}P(E) &\quad = P(H) \cdot P(E \mid H) + P(\hat{H})\cdot P(E\mid \hat{H}) \\\\&\quad= P(H,E) + P(\hat{H},E)\end{aligned}</script><script type="math/tex; mode=display">P(H\mid E) = \cfrac {P(H)P(E\mid H)}{P(H) \cdot P(E \mid H) + P(\hat{H})\cdot P(E\mid \hat{H})}</script><h2 id="Example-2-Drug-Testing"><a href="#Example-2-Drug-Testing" class="headerlink" title="Example 2: Drug Testing"></a>Example 2: Drug Testing</h2><p>Suppose that a drug test will produce 99% true positive results for drug users and 99% true negative results for non-drug users. Suppose that 0.5% of people are users of the drug. <strong>What is the probability that a randomly selected individual with a positive test is a drug user? What is the probability that a randomly selected individual with a negative test is a drug user?</strong></p><ul><li>Drug user: $drug$</li><li>Non-drug user: $\hat{drug}$</li><li>Positive: $+$</li><li>Negative: $-$</li></ul><h3 id="Question-1"><a href="#Question-1" class="headerlink" title="Question"></a>Question</h3><ul><li>$P(drug\mid +) = ?$</li><li>$P(drug\mid -) = ?$</li></ul><h3 id="Analyse-1"><a href="#Analyse-1" class="headerlink" title="Analyse"></a>Analyse</h3><p>$P(drug\mid +) = ?$</p><ul><li><p>$P(drug\mid +) = P(drug) \cdot P(+\mid drug) \:/\: P(+)$</p></li><li><p>$P(+) = P(drug) \cdot P(+\mid drug) + P(\hat{drug}) \cdot P(+ \mid \hat{drug})=1.49\%$</p></li><li><p>$P(drug\mid +) = 0.5\% \times 99\% \:/\: 1.49\% \approx 33.2\%$</p></li></ul><p>$P(drug\mid -) = ?$</p><ul><li><p>$P(drug\mid -) = P(drug) \cdot P(-\mid drug) \:/\: P(-)$</p></li><li><p>$P(-) = P(drug) \cdot P(-\mid drug) + P(\hat{drug}) \cdot P(- \mid \hat{drug}) = 98.51\%$</p></li><li><p>$P(drug\mid -) = 0.5\% \times 1\% \:/\: 98.51\% \approx 0.005\%$</p></li></ul><h2 id="Revisit-Drug-Testing-Extension"><a href="#Revisit-Drug-Testing-Extension" class="headerlink" title="Revisit Drug Testing - Extension"></a>Revisit Drug Testing - Extension</h2><p>Suppose that a drug test will produce 99% true positive results for drug users and 99% true negative results for non‐drug users. Suppose that 0.5% of people are users of the drug. <strong>What is the probability that a randomly selected individual with two positive tests is a drug user?</strong> </p><p><em>(assume that these two tests are mutually independent given whether if the person is a drug user or not)</em></p><h3 id="Question-2"><a href="#Question-2" class="headerlink" title="Question"></a>Question</h3><p>$P(drug\mid ++) = ?$</p><h3 id="Analyse-2"><a href="#Analyse-2" class="headerlink" title="Analyse"></a>Analyse</h3><ul><li><p>$P(drug\mid ++) = P(drug) \cdot P(++\mid drug) \:/\: P(++)$</p></li><li><p>$P(++\mid drug) = P(+\mid drug) \cdot P(+\mid drug) = 99\% \cdot 99\% = 98.01\%$</p></li><li><p>$P(++) = P(drug)\cdot P(++\mid drug) + P(\hat{drug})\cdot P(++\mid \hat{drug}) = 0.5\%$</p></li><li><p>$P(drug\mid ++) = 98.01\%$</p></li></ul><h2 id="Interpretation-of-Bayes-Theorem"><a href="#Interpretation-of-Bayes-Theorem" class="headerlink" title="Interpretation of Bayes Theorem"></a>Interpretation of Bayes Theorem</h2><ul><li>Probability measures a “degree of belief”</li><li>Bayes theorem links the degree of belief in a hypothesis (proposition) before and after accounting for evidence</li><li>$P(H \mid E) = P(H) \cdot P(E \mid H)\:/\:P(E)$, hypothesis $H$ and evidence $E$</li><li>$P(H)$ is the initial degree of belief in $H$ (i.e. prior probability)</li><li>$P(H\mid E)$ is the degree of belief having accounted for $E$</li><li>Bayes theorem is not to determine our belief, but to update our belief</li></ul><h2 id="Bayes-Theorem-for-Classification"><a href="#Bayes-Theorem-for-Classification" class="headerlink" title="Bayes Theorem for Classification"></a>Bayes Theorem for Classification</h2><ul><li>A machine learning model is to predict the relationships in the data, such as the relationship between features (i.e. attributes) (e.g. round, juicy, orange skin) and classes (e.g. orange, apple)</li><li>Bayes theorem provides a probabilistic way to model the relationship in data: evidence = feature (input), hypothesis = class (output)</li><li>It calculates the probability of an object given a number of<br>features (input $f_1,f_2,…, f_n$) belonging to a particular class (output $c$): $P(Y=c \mid X_1 = f_1, X_2 = f_2,…, X_n = f_n)$</li></ul><h2 id="Example-Tooth-cavity-testing"><a href="#Example-Tooth-cavity-testing" class="headerlink" title="Example: Tooth cavity testing"></a>Example: Tooth cavity testing</h2><ul><li>To predict whether people have cavity when they do not do mouth wash by Bayes theorem</li><li>Difference from the previous examples: unknown prior probability, likelihood, etc; need to learn from training examples</li></ul><h3 id="Analyse-3"><a href="#Analyse-3" class="headerlink" title="Analyse"></a>Analyse</h3><ul><li>Input variable: Wash</li><li>Output variable: Cavity</li></ul><h4 id="Training-Set"><a href="#Training-Set" class="headerlink" title="Training Set"></a>Training Set</h4><div class="table-container"><table><thead><tr><th style="text-align:center">Person</th><th style="text-align:center">Wash</th><th style="text-align:center">Cavity</th></tr></thead><tbody><tr><td style="text-align:center">P1</td><td style="text-align:center">no</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P2</td><td style="text-align:center">no</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P3</td><td style="text-align:center">yes</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P4</td><td style="text-align:center">yes</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P5</td><td style="text-align:center">yes</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P6</td><td style="text-align:center">no</td><td style="text-align:center">no</td></tr></tbody></table></div><h4 id="Frequency-Table"><a href="#Frequency-Table" class="headerlink" title="Frequency Table"></a>Frequency Table</h4><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">Cavity = no</th><th style="text-align:center">Cavity = yes</th><th style="text-align:center">Total</th></tr></thead><tbody><tr><td style="text-align:center">Wash = no</td><td style="text-align:center">1</td><td style="text-align:center">2</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">Wash = yes</td><td style="text-align:center">2</td><td style="text-align:center">1</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">Total</td><td style="text-align:center">3</td><td style="text-align:center">3</td><td style="text-align:center">6</td></tr></tbody></table></div><h4 id="Probabilities-based-on-frequency-table"><a href="#Probabilities-based-on-frequency-table" class="headerlink" title="Probabilities based on frequency table"></a>Probabilities based on frequency table</h4><ul><li><p>$P(\hat{wash} \mid vacity) = \cfrac{2}{3}$</p></li><li><p>$P(wash \mid vacity) = \cfrac{1}{3}$</p></li><li><p>$P(cavity) = \cfrac{3}{6}$</p></li><li><p>$P(\hat{cavity}) = \cfrac{3}{6}$</p></li><li><p>$P(wash) = \cfrac{3}{6}$</p></li><li><p>$P(\hat{wash}) = \cfrac{3}{6}$</p></li></ul><p>Now we can predict whether people have cavity when not doing mouth wash.</p><ul><li><p>$P(cavity \mid \hat{wash})<br> = P(cavity) \cdot P(\hat{wash}\mid cavity) \:/\: P(\hat{wash})<br> \approx 67\%$</p></li><li><p>$P(\hat{cavity} \mid \hat{wash}) \approx 33\%$</p></li></ul><h2 id="Bayes-Theorem-for-multiple-input-attributes"><a href="#Bayes-Theorem-for-multiple-input-attributes" class="headerlink" title="Bayes Theorem for multiple input attributes"></a>Bayes Theorem for multiple input attributes</h2><p>To predict whether people have cavity when they do not do<br>mouth wash and have pain: $P(Y=cavity\mid X1=\hat{wash}, X2=pain)$</p><ul><li>Input variables: Wash, Pain</li><li>Output variable: Cavity</li></ul><h3 id="Frequency-table-of-joint-input-attributes"><a href="#Frequency-table-of-joint-input-attributes" class="headerlink" title="Frequency table of joint input attributes"></a>Frequency table of joint input attributes</h3><h4 id="Training-Set-1"><a href="#Training-Set-1" class="headerlink" title="Training Set"></a>Training Set</h4><div class="table-container"><table><thead><tr><th style="text-align:center">Person</th><th style="text-align:center">Wash</th><th style="text-align:center">Pain</th><th style="text-align:center">Cavity</th></tr></thead><tbody><tr><td style="text-align:center">P1</td><td style="text-align:center">no</td><td style="text-align:center">yes</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P2</td><td style="text-align:center">no</td><td style="text-align:center">yes</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P3</td><td style="text-align:center">yes</td><td style="text-align:center">yes</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P4</td><td style="text-align:center">yes</td><td style="text-align:center">no</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P5</td><td style="text-align:center">yes</td><td style="text-align:center">no</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P6</td><td style="text-align:center">no</td><td style="text-align:center">no</td><td style="text-align:center">no</td></tr></tbody></table></div><h4 id="Frequency-Table-1"><a href="#Frequency-Table-1" class="headerlink" title="Frequency Table"></a>Frequency Table</h4><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">Cavity = no</th><th style="text-align:center">Cavity = yes</th><th style="text-align:center">Total</th></tr></thead><tbody><tr><td style="text-align:center">Wash = no, Pain = no</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">Wash = no, Pain = yes</td><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">Wash = yes, Pain = no</td><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">Wash = yes, Pain = yes</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">Total</td><td style="text-align:center">3</td><td style="text-align:center">3</td><td style="text-align:center">6</td></tr></tbody></table></div><script type="math/tex; mode=display">\begin{aligned} &P(cavity \mid \hat{wash}, pain) \\\\= \quad &\cfrac{P(cavity) \cdot P(\hat{wash}, pain \mid cavity)}{P(\hat{wash}, pain)} \\\\= \quad &  \cfrac{P(cavity)\cdot P(\hat{wash}, pain\mid cavity)}{(P(cavity) \cdot P(\hat{wash}, pain \mid cavity) + P(\hat{cavity}) \cdot P(\hat{wash}, pain\mid \hat{cavity}))}\\\\= \quad & \cfrac{\cfrac{3}{6} \times \cfrac{2}{3}}   {\cfrac{3}{6} \times \cfrac{2}{3} + \cfrac{3}{6} \times 0}  \\\\= \quad & 100\%\end{aligned}</script><ul><li><strong>Downside</strong>: Number of possible combinations of input attributes can become very large</li></ul><h2 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h2><p>Naive Bayes = Bayes Theorem + Independence Assumption</p><ul><li><p>Bayes Theorem: </p><script type="math/tex; mode=display">P(c \mid f_1, ..., f_n) = \cfrac{P( c )  \cdot P(f_1, ..., f_n \mid c)}{P(f_1, ..., f_n)}</script></li><li><p>Independence Assumption: Input attributes $f_1, …, f_n$ are mutually independent, conditional on any output, that is </p></li></ul><script type="math/tex; mode=display">P(f_1, ..., f_n \mid c) = P(f_1 \mid c) \cdot P(f_2 \mid c) ... P(f_n \mid c)</script><ul><li>Naive Bayes</li></ul><script type="math/tex; mode=display">P(c \mid f_1, ..., f_n) = \cfrac{P( c ) \cdot P(f_1 \mid c) \cdot P(f_2 \mid c) ... P(f_n \mid c)}{P(f_1, ..., f_n)}</script><p>If there are m possible classes: $c_1, c_2, …, c_m$, based on the independent assumption we have</p><script type="math/tex; mode=display">P(f_1, ..., f_n) =    \sum_{j=1}^{m} P(c_j)    \cdot \prod _{i=1}^{n}P(f_1 \mid c_j)</script><p>As $P(f_1, …, f_n)$ is the same (i.e. a constant) for any class $c$, we can denote $P(f_1, …, f_n) = \cfrac{1}{\alpha}$,<br>where $\alpha$ is a normalised factor</p><script type="math/tex; mode=display">\alpha = \cfrac{1}{\sum_{j=1}^{m} P(c_j) \cdot \prod _{i=1}^{n}P(f_1 \mid c_j)}</script><p>Therefore </p><script type="math/tex; mode=display">P(c \mid f_1, ..., f_n) = \alpha \cdot P( c ) \cdot P(f_1 \mid c)   \cdot P(f_2 \mid c) ... P(f_n \mid c)</script><h3 id="Frequency-table-of-joint-input-attributes-1"><a href="#Frequency-table-of-joint-input-attributes-1" class="headerlink" title="Frequency table of joint input attributes"></a>Frequency table of joint input attributes</h3><h4 id="Training-Set-2"><a href="#Training-Set-2" class="headerlink" title="Training Set"></a>Training Set</h4><div class="table-container"><table><thead><tr><th style="text-align:center">Person</th><th style="text-align:center">Wash</th><th style="text-align:center">Pain</th><th style="text-align:center">Cavity</th></tr></thead><tbody><tr><td style="text-align:center">P1</td><td style="text-align:center">no</td><td style="text-align:center">yes</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P2</td><td style="text-align:center">no</td><td style="text-align:center">yes</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P3</td><td style="text-align:center">yes</td><td style="text-align:center">yes</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P4</td><td style="text-align:center">yes</td><td style="text-align:center">no</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P5</td><td style="text-align:center">yes</td><td style="text-align:center">no</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P6</td><td style="text-align:center">no</td><td style="text-align:center">no</td><td style="text-align:center">no</td></tr></tbody></table></div><h4 id="Frequency-Table-2"><a href="#Frequency-Table-2" class="headerlink" title="Frequency Table"></a>Frequency Table</h4><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">Cavity = no</th><th style="text-align:center">Cavity = yes</th><th style="text-align:center">Total</th></tr></thead><tbody><tr><td style="text-align:center">Wash = no, Pain = no</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">Wash = no, Pain = yes</td><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">Wash = yes, Pain = no</td><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">Wash = yes, Pain = yes</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">Total</td><td style="text-align:center">3</td><td style="text-align:center">3</td><td style="text-align:center">6</td></tr></tbody></table></div><script type="math/tex; mode=display">\begin{aligned}P(c \mid f_1, ..., f_n) &= \alpha \cdot P( c ) \cdot P(f_1 \mid c)   \cdot P(f_2 \mid c) ... P(f_n \mid c)\\\\&= \alpha \cdot P( c ) \cdot \prod _{i=1}^{n}P(f_i \mid c)\end{aligned}</script><p>Therefore</p><script type="math/tex; mode=display">P(cavity \mid \hat{wash}, pain) = \alpha \cdot P( cavity ) \cdot P(\hat{wasgh} \mid cavity)   \cdot P(pain \mid cavity)</script><h4 id="Frequency-Table-3"><a href="#Frequency-Table-3" class="headerlink" title="Frequency Table"></a>Frequency Table</h4><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">Cavity = no</th><th style="text-align:center">Cavity = yes</th><th style="text-align:center">Total</th></tr></thead><tbody><tr><td style="text-align:center">Wash = no</td><td style="text-align:center">1</td><td style="text-align:center">2</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">Wash = yes</td><td style="text-align:center">2</td><td style="text-align:center">1</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">Total</td><td style="text-align:center">3</td><td style="text-align:center">3</td><td style="text-align:center">6</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">Cavity = no</th><th style="text-align:center">Cavity = yes</th><th style="text-align:center">Total</th></tr></thead><tbody><tr><td style="text-align:center">Pain = no</td><td style="text-align:center">3</td><td style="text-align:center">0</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">Pain = yes</td><td style="text-align:center">0</td><td style="text-align:center">3</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">Total</td><td style="text-align:center">3</td><td style="text-align:center">3</td><td style="text-align:center">6</td></tr></tbody></table></div><h3 id="Naive-Bayes-classifier"><a href="#Naive-Bayes-classifier" class="headerlink" title="Naive Bayes classifier"></a>Naive Bayes classifier</h3><script type="math/tex; mode=display">\begin{aligned}&P(cavity \mid \hat{wash}, pain) \\\\= \quad &\alpha \cdot P( cavity ) \cdot P(\hat{wash} \mid cavity)   \cdot P(pain \mid cavity) \\\\= \quad &\alpha \times \cfrac{3}{6} \times \cfrac{2}{3} \times \cfrac{3}{3} \\\\= \quad &\alpha  \times \cfrac{1}{3}\\\\= \quad &100 \%\end{aligned}</script><p>and</p><script type="math/tex; mode=display">\begin{aligned}&P(\hat{cavity} \mid \hat{wash}, pain) \\\\= \quad &\alpha \cdot P( \hat{cavity} ) \cdot P(\hat{wash} \mid \hat{cavity})   \cdot P(pain \mid \hat{cavity}) \\\\= \quad &\alpha \times \cfrac{3}{6} \times \cfrac{1}{3} \times \cfrac{0}{3} \\\\= \quad &\alpha  \times 0\\\\= \quad &0 \%\end{aligned}</script><p>Where</p><script type="math/tex; mode=display">\begin{aligned}\alpha &=\cfrac{1}{P(cavity) \cdot P(\hat{wash}\mid cavity)\cdot P(pain\mid cavity) + P(\hat{cavity})\cdot P(\hat{wash}\mid \hat{cavity})\cdot P(pain\mid \hat{cavity})} \\\\ &=3\end{aligned}</script><h2 id="Categorical-naive-Bayes"><a href="#Categorical-naive-Bayes" class="headerlink" title="Categorical naive Bayes"></a>Categorical naive Bayes</h2><p><strong>Categorical naive Bayes</strong>: all the input attributes are categorical variables.</p><ul><li>A categorical variable is a variable that can take on one of a limited number of possible values</li><li>This contrasts with numerical (continuous) naive Bayes, where<br>some input variable can be represented by real numbers (i.e. infinite possible attribute values)</li></ul><h3 id="Categorical-vs-Numerical-attributes"><a href="#Categorical-vs-Numerical-attributes" class="headerlink" title="Categorical vs Numerical attributes"></a>Categorical vs Numerical attributes</h3><h4 id="Training-Set-3"><a href="#Training-Set-3" class="headerlink" title="Training Set"></a>Training Set</h4><div class="table-container"><table><thead><tr><th style="text-align:center">Person</th><th style="text-align:center">Wash</th><th style="text-align:center">Pain</th><th style="text-align:center">Cavity</th></tr></thead><tbody><tr><td style="text-align:center">P1</td><td style="text-align:center">no</td><td style="text-align:center">yes</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P2</td><td style="text-align:center">no</td><td style="text-align:center">yes</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P3</td><td style="text-align:center">yes</td><td style="text-align:center">yes</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P4</td><td style="text-align:center">yes</td><td style="text-align:center">no</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P5</td><td style="text-align:center">yes</td><td style="text-align:center">no</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P6</td><td style="text-align:center">no</td><td style="text-align:center">no</td><td style="text-align:center">no</td></tr></tbody></table></div><h4 id="Categorical"><a href="#Categorical" class="headerlink" title="Categorical"></a>Categorical</h4><div class="table-container"><table><thead><tr><th style="text-align:center">Person</th><th style="text-align:center">Wash</th><th style="text-align:center">Cavity</th></tr></thead><tbody><tr><td style="text-align:center">P1</td><td style="text-align:center">no</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P2</td><td style="text-align:center">no</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P3</td><td style="text-align:center">yes</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P4</td><td style="text-align:center">yes</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P5</td><td style="text-align:center">yes</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P6</td><td style="text-align:center">no</td><td style="text-align:center">no</td></tr></tbody></table></div><h4 id="Numerical"><a href="#Numerical" class="headerlink" title="Numerical"></a>Numerical</h4><div class="table-container"><table><thead><tr><th style="text-align:center">Person</th><th style="text-align:center">Sugar (grams)</th><th style="text-align:center">Cavity</th></tr></thead><tbody><tr><td style="text-align:center">P1</td><td style="text-align:center">40</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P2</td><td style="text-align:center">35</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P3</td><td style="text-align:center">60</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P4</td><td style="text-align:center">20</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P5</td><td style="text-align:center">30</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P6</td><td style="text-align:center">17</td><td style="text-align:center">no</td></tr></tbody></table></div><h3 id="Deal-with-numerical-input-attributes-by-discretising-values"><a href="#Deal-with-numerical-input-attributes-by-discretising-values" class="headerlink" title="Deal with numerical input attributes by discretising values"></a>Deal with numerical input attributes by discretising values</h3><ul><li><p>Transform numerical input values into categories (i.e., discrete values)</p></li><li><p>E.g.: Sugar $\rightarrow$ small, medium, large</p></li><li><p>However, a person may consider large as &gt;30g. Another may consider it as &gt;25g</p></li><li><p>If the amount of sugar is large when $\ge$ 30g, should 29.9g be considered medium? Not large?</p></li><li><p>This level of subjectiveness and crispiness may lead to loss of information</p></li></ul><h3 id="Probability-Function"><a href="#Probability-Function" class="headerlink" title="Probability Function"></a>Probability Function</h3><p>Probability Function describes the probability distribution of a random variable, i.e., the probabilities of occurrence of different possible outcomes in an experiment.</p><p><strong>Probability Mass Function</strong>: the random variable is discrete</p><div class="table-container"><table><thead><tr><th style="text-align:center">Person</th><th style="text-align:center">Wash</th><th style="text-align:center">Cavity</th></tr></thead><tbody><tr><td style="text-align:center">P1</td><td style="text-align:center">no</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P2</td><td style="text-align:center">no</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P3</td><td style="text-align:center">yes</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P4</td><td style="text-align:center">yes</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P5</td><td style="text-align:center">yes</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P6</td><td style="text-align:center">no</td><td style="text-align:center">no</td></tr></tbody></table></div><p>Probability Mass Function for P(Wash|cavity):<br><img src="/2020/04/02/Intro-to-AI-03/02.png" style="zoom:75%;"></p><ul><li>X axis represents possible values of the feature</li><li>Y axis represents the probability of the feature being a value given a particular class</li></ul><h3 id="Probability-Density-Function"><a href="#Probability-Density-Function" class="headerlink" title="Probability Density Function"></a>Probability Density Function</h3><p>Probability Density Function is a probability function where the variable is continuous.</p><div class="table-container"><table><thead><tr><th style="text-align:center">Person</th><th style="text-align:center">Sugar (grams)</th><th style="text-align:center">Cavity</th></tr></thead><tbody><tr><td style="text-align:center">P1</td><td style="text-align:center">40</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P2</td><td style="text-align:center">35</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P3</td><td style="text-align:center">60</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P4</td><td style="text-align:center">20</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P5</td><td style="text-align:center">30</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P6</td><td style="text-align:center">17</td><td style="text-align:center">no</td></tr></tbody></table></div><ul><li>Shape of the function: Assume the examples drawn from Gaussian Distribution (aka normal Distribution).</li><li>Shape is controlled by two parameters: Mean ($\mu$) and Standard Deviation ($\sigma$), determined by the training examples.</li></ul><p>Gaussian Distribution $N(x \mid \mu , \sigma )$:<br><img src="/2020/04/02/Intro-to-AI-03/03.png" style="zoom:75%;"></p><h4 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h4><p><img src="/2020/04/02/Intro-to-AI-03/04.png" style="zoom:50%;"></p><p><img src="/2020/04/02/Intro-to-AI-03/05.png" style="zoom:50%;"></p><h4 id="Probability-Density-Function-calculation-for-people-having-cavity"><a href="#Probability-Density-Function-calculation-for-people-having-cavity" class="headerlink" title="Probability Density Function calculation for people having cavity"></a>Probability Density Function calculation for people having cavity</h4><p>$x_i$ is the amount of the sugar person takes, where is the number of the people having cavity in the training examples.</p><p><img src="/2020/04/02/Intro-to-AI-03/06.png" style="zoom:75%;"></p><script type="math/tex; mode=display">\mu = \cfrac{\sum_{i=1}^{n} x_i}{n}</script><script type="math/tex; mode=display">\sigma = \sqrt{\cfrac{\sum_{i=1}^{n} (x_i - \mu)^2}{n}}</script><p>Therefore</p><script type="math/tex; mode=display">\mu = \cfrac{40 + 35 + 60}{3} = 45</script><script type="math/tex; mode=display">\begin{aligned}\sigma ^2 &= \cfrac{1}{3} \times[(40-45)^2 + (35-45)^2 + (60-45)^2] \\\\&= \cfrac{1}{3} \times [25 + 100 + 225] \\\\&\approx 116.67\end{aligned}</script><p>Use $\mu$ and $\sigma$ to plot the Gaussian distribution graph:</p><p>Sugar is $x$ and $P(Sugar\mid cavity)$ is $N(x \mid \mu, \sigma)$.</p><p><img src="/2020/04/02/Intro-to-AI-03/07.png" style="zoom:50%;"></p><script type="math/tex; mode=display">N(x \mid \mu, \sigma) = \cfrac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^2}{2\sigma ^2}}</script><script type="math/tex; mode=display">\mu=45</script><script type="math/tex; mode=display">\sigma ^2 = 116.67</script><h4 id="Probability-Density-Function-calculation-for-people-not-having-cavity"><a href="#Probability-Density-Function-calculation-for-people-not-having-cavity" class="headerlink" title="Probability Density Function calculation for people not having cavity"></a>Probability Density Function calculation for people not having cavity</h4><p>$x_i$ is the amount of the sugar person takes, where is the number of the people not having cavity in the training examples.</p><p><img src="/2020/04/02/Intro-to-AI-03/08.png" style="zoom:75%;"></p><script type="math/tex; mode=display">\mu = \cfrac{\sum_{i=1}^{n} x_i}{n}</script><script type="math/tex; mode=display">\sigma = \sqrt{\cfrac{\sum_{i=1}^{n} (x_i - \mu)^2}{n}}</script><p>Therefore</p><script type="math/tex; mode=display">\mu = \cfrac{20 + 30 + 17}{3} \approx 22.33</script><script type="math/tex; mode=display">\sigma ^2  \approx 30.89</script><p>Use $\mu$ and $\sigma$ to plot the Gaussian distribution graph:</p><p>Sugar is $x$ and $P(Sugar\mid \hat{cavity})$ is $N(x \mid \mu, \sigma)$.</p><p><img src="/2020/04/02/Intro-to-AI-03/09.png" style="zoom:50%;"></p><script type="math/tex; mode=display">N(x \mid \mu, \sigma) = \cfrac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^2}{2\sigma ^2}}</script><script type="math/tex; mode=display">\mu=22.33</script><script type="math/tex; mode=display">\sigma ^2 = 30.89</script><h4 id="Probability-distribution-functions"><a href="#Probability-distribution-functions" class="headerlink" title="Probability distribution functions"></a>Probability distribution functions</h4><p>Probability distribution function of Sugar given cavity or no cavity:</p><div class="table-container"><table><thead><tr><th style="text-align:center">Person</th><th style="text-align:center">Sugar (grams)</th><th style="text-align:center">Cavity</th></tr></thead><tbody><tr><td style="text-align:center">P1</td><td style="text-align:center">40</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P2</td><td style="text-align:center">35</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P3</td><td style="text-align:center">60</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P4</td><td style="text-align:center">20</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P5</td><td style="text-align:center">30</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P6</td><td style="text-align:center">17</td><td style="text-align:center">no</td></tr></tbody></table></div><p><img src="/2020/04/02/Intro-to-AI-03/10.png" style="zoom:50%;"></p><h3 id="General-idea"><a href="#General-idea" class="headerlink" title="General idea"></a>General idea</h3><ul><li><p>Decide on a type of probability density function (usually Gaussian distribution) for each numerical input attribute $X$.</p></li><li><p>Determine the parameters of the probability function for each possible output attribute value (i.e.$c_1, c_2, …, c_m$) according to the training examples.</p></li><li><p>Use the obtained probability function for $c_i$ to calculate $P(X =f \mid Y = c_i)$ for the corresponding input value $f$.</p></li></ul><h2 id="Example-1"><a href="#Example-1" class="headerlink" title="Example"></a>Example</h2><ul><li><p>We want to predict if a person has cavity when they do not do mouth wash and take sugar 20 grams everyday.</p></li><li><p>We need to calculate $P(cavity \mid \hat{wash}, Sugar=20)$ and $P(\hat{cavity}\mid \hat{wash}, Sugar=20)$.</p></li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">Person</th><th style="text-align:center">Wash</th><th style="text-align:center">Sugar (grams)</th><th style="text-align:center">Cavity</th></tr></thead><tbody><tr><td style="text-align:center">P1</td><td style="text-align:center">no</td><td style="text-align:center">40</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P2</td><td style="text-align:center">no</td><td style="text-align:center">35</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P3</td><td style="text-align:center">yes</td><td style="text-align:center">60</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">P4</td><td style="text-align:center">yes</td><td style="text-align:center">20</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P5</td><td style="text-align:center">yes</td><td style="text-align:center">30</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">P6</td><td style="text-align:center">no</td><td style="text-align:center">17</td><td style="text-align:center">no</td></tr></tbody></table></div><script type="math/tex; mode=display">\begin{aligned}&P(cavity \mid \hat{wash}, Sugar=20)\\\\= \quad & \alpha \cdot P(cavity) \cdot P(\hat{wash} \mid cavity) \cdot P(Sugar = 20 \mid cavity) \\\\= \quad & \alpha \times \cfrac{3}{6} \times \cfrac{2}{3} \times P(Sugar = 20 \mid cavity)\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}&P(\hat{cavity} \mid \hat{wash}, Sugar=20)\\\\= \quad & \alpha \cdot P(\hat{cavity}) \cdot P(\hat{wash} \mid \hat{cavity}) \cdot P(Sugar = 20 \mid \hat{cavity}) \\\\= \quad & \alpha \times \cfrac{3}{6} \times \cfrac{1}{3} \times P(Sugar = 20 \mid \hat{cavity})\end{aligned}</script><p>Calculate $P(Sugar=20 \mid cavity)$:</p><ul><li>Probability Density Function of the variable Sugar given cavity</li></ul><p><img src="/2020/04/02/Intro-to-AI-03/11.png" style="zoom:50%;"></p><p>Calculate $P(Sugar=20 \mid \hat{cavity})$:</p><ul><li>Probability Density Function of the variable Sugar given no cavity</li></ul><p><img src="/2020/04/02/Intro-to-AI-03/12.png" style="zoom:50%;"></p><h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><p>Predict if people have cavity when they do not do mouth wash and take sugar 20 grams everyday.<br>$P(cavity \mid \hat{wash}, Sugar=20)<br>= \alpha \times \cfrac{3}{6} \times \cfrac{2}{3} \times0.0025 = 0.00083\alpha$<br>$P(\hat{cavity} \mid \hat{wash}, Sugar=20)<br>= \alpha \times \cfrac{3}{6} \times \cfrac{1}{3} \times0.0657 = 0.01095\alpha$</p><p><strong>Predicted results: No cavity</strong></p><h2 id="Advantages-and-disadvantages-of-naive-Bayes"><a href="#Advantages-and-disadvantages-of-naive-Bayes" class="headerlink" title="Advantages and disadvantages of naive Bayes"></a>Advantages and disadvantages of naive Bayes</h2><ul><li><p>Advantages:</p><ul><li>Does very well in classification, especially on high‐dimensional dataset</li><li>Requires a small amount of training data; training is very quick</li><li>Easy to build/implement</li></ul></li><li><p>Disadvantages:</p><ul><li>Requires the assumption of independent input attributes</li><li>For continuous input attributes, it assumes a certain probability distribution for input attributes</li><li>Not work very well for regression</li></ul></li></ul><h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h2><ul><li><p>To categorise emails, e.g., spam or not spam</p></li><li><p>To check a piece of text expressing positive emotions, or negative emotions</p></li><li><p>To classify a news article about technology, politics, or sports</p></li><li><p>Software defect prediction</p></li><li><p>Medical diagnosis</p></li><li><p>…</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Bayes-Theorem-and-Naive-Bayes&quot;&gt;&lt;a href=&quot;#Bayes-Theorem-and-Naive-Bayes&quot; class=&quot;headerlink&quot; title=&quot;Bayes Theorem and Naive Bayes&quot;&gt;&lt;/a
      
    
    </summary>
    
    
      <category term="AI" scheme="http://yoursite.com/categories/AI/"/>
    
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
      <category term="Bayes" scheme="http://yoursite.com/tags/Bayes/"/>
    
  </entry>
  
</feed>
